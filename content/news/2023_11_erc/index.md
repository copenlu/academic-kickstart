+++
title = "PhD position available in context of ERC Starting Grant project ExplainYourself"
date = 2023-11-07  # Schedule page publish date.
draft = false

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
time_start = 2023-11-07
#time_end = 2022-11-09
# all_day = true

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = []

# Abstract and optional shortened version.
abstract = ""
abstract_short = "A PhD position on explainable natural language understanding is available in CopeNLU. The positions is funded by the ERC Starting Grant project ExplainYourself, and applications are due by 1 February 2024."

# Name of event and optional event URL.
event = "ERC StG PhD Fellowship Call"
event_url = "https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&departmentId=18970&ProjectId=160498&MediaId=5&SkipAdvertisement=false"

# Location of event.
location = "Copenhagen, Denmark"

# Is this a featured talk? (true/false)
featured = true

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["natural language understanding", "fact-checking", "explainability"]

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Links (optional).
url_pdf = ""
url_slides = ""
url_video = ""
url_code = ""

# Does the content use math formatting?
math = false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# [image]
  # Caption (optional)

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Right"
+++
A PhD fellowship on explainable natural language understanding is available in CopeNLU. The successful candidate will be supervised by <a href="http://isabelleaugenstein.github.io/">Isabelle Augenstein</a> and <a href="https://apepa.github.io/">Pepa Atanasova</a>. The positions are offered in the context of an <a href="https://erc.europa.eu/apply-grant/starting-grant">ERC Starting Grant</a> on '<a href="https://erc.europa.eu/news/erc-2021-starting-grants-results">Explainable and Robust Automatic Fact Checking (ExplainYourself)</a>'. ERC Starting Grant is a highly competitive funding program by the <a href="https://erc.europa.eu/homepage">European Research Council</a> to support the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.

ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction. Automatic fact checking methods often use opaque deep neural network models, whose inner workings cannot easily be explained. Especially for complex tasks such as automatic fact checking, this hinders greater adoption, as it is unclear to users when the models' predictions can be trusted. Existing explainable ML methods partly overcome this by reducing the task of explanation generation to highlighting the right rationale. While a good first step, this does not fully explain how a ML model arrived at a prediction. For knowledge intensive natural language understanding (NLU) tasks such as fact checking, a ML model needs to learn complex relationships between the claim, multiple evidence documents, and common sense knowledge in addition to retrieving the right evidence. There is currently no explainability method that aims to illuminate this highly complex process. In addition, existing approaches are unable to produce diverse explanations, geared towards users with different information needs. ExplainYourself radically departs from existing work in proposing methods for explainable fact checking that more accurately reflect how fact checking models make decisions, and are useful to diverse groups of end users. It is expected that these innovations will apply to explanation generation for other knowledge-intensive NLU tasks, such as question answering or entity linking. 

In addition to the principle investigator, PhD students and postdocs, the project team will also include collaborators from CopeNLU as well as external collaborators. Two PhD students as well as a postdoc have already been recruited as a result of earlier calls, and the project officially <a href="/talk/2023_09_explainyourself/">kicked off in September 2023</a>.

Read more about reasons to join us <a href="/post/why-ucph/">here</a>. You can read more about the position and apply <a href="https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&departmentId=18970&ProjectId=160498&MediaId=5&SkipAdvertisement=false">here</a>.
