+++
title = "Factuality Challenges in the Era of Large Language Models"
date = 2024-07-10T00:00:00
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Isabelle Augenstein", "Timothy Baldwin", "Meeyoung Cha", "Tanmoy Chakraborty", "Giovanni Luca Ciampaglia", "David Corney", "Renee DiResta", "Emilio Ferrara", "Scott Hale", "Alon Halevy", "Eduard Hovy", "Heng Ji", "Filippo Menczer", "Ruben Miguez", "Preslav Nakov", "Dietram Scheufele", "Shivam Sharma", "Giovanni Zagni"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["2"]

# Publication name and optional abbreviated version.
publication = "Nature Machine Intelligence"
publication_short = "In *Nature Machine Intelligence*"

# Abstract and optional shortened version.
abstract = "The emergence of tools based on large language models (LLMs), like OpenAI’s ChatGPT and Google’s Gemini, has garnered immense public attention due to their advanced natural language generation capabilities. These remarkably natural-sounding tools have the potential to be highly useful across various tasks. However, they also tend to produce false, erroneous, or misleading content -- commonly referred to as hallucinations. Additionally, LLMs can be misused to generate convincing yet false content and profiles on a large scale, posing a substantial societal challenge by potentially deceiving users and spreading inaccurate information. This makes fact-checking increasingly important. Despite their issues with factual accuracy, LLMs have shown proficiency in various subtasks that support fact-checking, which is essential for ensuring factually accurate responses. In light of these concerns, we explore the issues related to factuality in LLMs and their impact on fact-checking. We identify key challenges, imminent threats, and possible solutions to these factuality issues. We also thoroughly examine these challenges, existing solutions, and potential prospects for fact-checking. By analysing the factuality constraints within LLMs and their impact on fact-checking, we aim to contribute to a path towards maintaining accuracy at a time of confluence of generative AI and misinformation."
abstract_short = ""

# Is this a featured publication? (true/false)
featured = true

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["fact-checking", "explainability"]

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides = "example-slides"

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = "https://www.nature.com/articles/s42256-024-00881-z"
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Digital Object Identifier (DOI)
doi = ""

# Does this page contain LaTeX math? (true/false)
math = false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
preview_only = true

  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Center"
+++


