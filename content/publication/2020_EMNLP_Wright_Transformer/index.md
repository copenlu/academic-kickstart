+++
title = "Transformer Based Multi-Source Domain Adaptation"
date = 2020-09-15T00:00:00
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Dustin Wright", "Isabelle Augenstein"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["1"]

# Publication name and optional abbreviated version.
publication = "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
publication_short = "In *EMNLP*"

# Abstract and optional shortened version.
abstract = "In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data  it  was  trained  on.   Here,  we  investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled  data  from  multiple  source  domainsand  must  make  predictions  on  a  domain  forwhich  no  labelled  data  has  been  seen.   Prior work with CNNs and RNNs has demonstrated the  benefit  of  mixture  of  experts,  where  the predictions  of  multiple  domain  expert  classifiers are combined;  as well as domain adversarial  training,  to  induce  a  domain  agnostic representation space.  Inspired by this, we investigate how such methods can be effectively applied  to  large  pretrained  transformer  models.    We  find  that  domain  adversarial  training  has  an  effect  on  the  learned  represen-tations  of  these  models  while  having  little effect  on  their  performance,  suggesting  that large transformer-based models are already relatively  robust  across  domains.   Additionally, we show that mixture of experts leads to significant  performance  improvements  by  comparing  several  variants  of  mixing  functions,  including one novel mixture based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective functions for mixing their predictions."
abstract_short = ""

# Is this a featured publication? (true/false)
featured = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["limited-data"]

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides = "example-slides"

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = ""
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Digital Object Identifier (DOI)
doi = ""

# Does this page contain LaTeX math? (true/false)
math = false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
preview_only = true

  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Center"
+++


